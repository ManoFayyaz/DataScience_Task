{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b3dbd7f",
   "metadata": {},
   "source": [
    "NLP Machine Learning Pipeline for Twitter Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bf6c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96b9e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "print(nltk.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa9662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9844e8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4116953",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('twitter_training.csv',header=None,names=['id','topic','label','review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9761dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        0\n",
       "topic     0\n",
       "label     0\n",
       "review    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(df)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e80c2f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73996, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46ba11ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd81c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['review']\n",
    "y=df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54092770",
   "metadata": {},
   "source": [
    "Cleaning text:\n",
    "\n",
    "Lowercase\n",
    "\n",
    "Remove punctuation\n",
    "\n",
    "break sentence into words\n",
    "\n",
    "remove stopwords\n",
    "\n",
    "convert words into their base form\n",
    "\n",
    "join text into a clean sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82c6532",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "644d36f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    tokens=word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords.words('english')]\n",
    "    # tokens = [ for word in tokens]\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96119bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_review']=df['review'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c28de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty rows\n",
    "# df = df[df[\"clean_text\"].str.strip() != \"\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3597eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    im getting borderland murder\n",
      "1              coming border kill\n",
      "2      im getting borderland kill\n",
      "3     im coming borderland murder\n",
      "4    im getting borderland murder\n",
      "Name: clean_review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['clean_review'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc2a1165",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(df['clean_review'],df['label'],test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f8587e",
   "metadata": {},
   "source": [
    "TF- Term Frequency: how many time a word repeated itself?\n",
    "\n",
    "IDF- Inverse Document Frequency: how rare a word is?\n",
    "\n",
    "Higher score to the words that are frequent but overall rare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ceab464",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer()\n",
    "X_train_vec=vectorizer.fit_transform(X_train)\n",
    "X_test_vec=vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55034d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7170657873398562\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.95      0.42      0.58      3348\n",
      "    Negative       0.64      0.90      0.75      5477\n",
      "     Neutral       0.84      0.62      0.71      4528\n",
      "    Positive       0.69      0.80      0.74      5146\n",
      "\n",
      "    accuracy                           0.72     18499\n",
      "   macro avg       0.78      0.69      0.70     18499\n",
      "weighted avg       0.76      0.72      0.71     18499\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1394  952  211  791]\n",
      " [  23 4909  165  380]\n",
      " [  27 1008 2822  671]\n",
      " [  18  808  180 4140]]\n",
      "F1 Score: 0.7076071217200118\n"
     ]
    }
   ],
   "source": [
    "model=MultinomialNB()\n",
    "model.fit(X_train_vec,y_train)\n",
    "y_pred=model.predict(X_test_vec)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test,y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test,y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test,y_pred))\n",
    "print('F1 Score:', f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "916f6405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy 0.7751590176045552\n",
      "test_accuracy 0.7170657873398562\n"
     ]
    }
   ],
   "source": [
    "print(\"train_accuracy\",accuracy_score(y_train,model.predict(X_train_vec)))\n",
    "print(\"test_accuracy\",accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe84891",
   "metadata": {},
   "source": [
    "NLP Learning Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d719c888",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8d4d3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Welcome, myself Mehnoor Fayyaz, a student of software engineering.\n",
      "Currently in my final year and working on my Final Year Project. I'm\n",
      "also learning Natural Language Processing and its applications.\n"
     ]
    }
   ],
   "source": [
    "corpus=\"\"\"Hello Welcome, myself Mehnoor Fayyaz, a student of software engineering.\n",
    "Currently in my final year and working on my Final Year Project. I'm\n",
    "also learning Natural Language Processing and its applications.\"\"\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f2e9fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append(\"C:/Users/mehno/AppData/Roaming/nltk_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "456bf422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\mehno/nltk_data', 'c:\\\\Users\\\\mehno\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data', 'c:\\\\Users\\\\mehno\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data', 'c:\\\\Users\\\\mehno\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\mehno\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data', 'C:/Users/mehno/AppData/Roaming/nltk_data', 'C:\\\\Users\\\\mehno\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\\\\\Users\\\\\\\\mehno\\\\\\\\AppData\\\\\\\\Roaming\\\\\\\\nltk_data', 'C:/Users/mehno/AppData/Roaming/nltk_data']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f09034b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehno\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8d88fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New paths: ['C:\\\\Users\\\\mehno\\\\AppData\\\\Roaming\\\\nltk_data']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.data.path = []\n",
    "\n",
    "# Add ONLY the correct NLTK folder\n",
    "nltk.data.path.append(r\"C:\\Users\\mehno\\AppData\\Roaming\\nltk_data\")\n",
    "\n",
    "print(\"New paths:\", nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "daae4ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehno\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\mehno\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt', force=True, download_dir=r\"C:\\Users\\mehno\\AppData\\Roaming\\nltk_data\")\n",
    "nltk.download('punkt_tab', force=True, download_dir=r\"C:\\Users\\mehno\\AppData\\Roaming\\nltk_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c594e2d4",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f08bc4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Welcome, myself Mehnoor Fayyaz, a student of software engineering.', 'Currently in my final year and working on my Final Year Project.', \"I'm\\nalso learning Natural Language Processing and its applications.\"]\n"
     ]
    }
   ],
   "source": [
    "#Tokenization\n",
    "#sent tokenize: paragraphs --> sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def sent_tokenize_corpus(corpus,language='english'):\n",
    "    return sent_tokenize(corpus, language=language) \n",
    "\n",
    "documents=sent_tokenize_corpus(corpus)\n",
    "print(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4597cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Welcome', ',', 'myself', 'Mehnoor', 'Fayyaz', ',', 'a', 'student', 'of', 'software', 'engineering', '.', 'Currently', 'in', 'my', 'final', 'year', 'and', 'working', 'on', 'my', 'Final', 'Year', 'Project', '.', 'I', \"'m\", 'also', 'learning', 'Natural', 'Language', 'Processing', 'and', 'its', 'applications', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokens=word_tokenize(corpus)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b705c9a1",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "708930f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=['eating','eaten','eats','programming','programmed','programmer','program','finally','finalized','final']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05ffcd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemming=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e020ad06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating --> eat\n",
      "eaten --> eaten\n",
      "eats --> eat\n",
      "programming --> program\n",
      "programmed --> program\n",
      "programmer --> programm\n",
      "program --> program\n",
      "finally --> final\n",
      "finalized --> final\n",
      "final --> final\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word,'-->',stemming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29921079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratul'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem('congratulations')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94e2b02",
   "metadata": {},
   "source": [
    "Regular Expression can take a singular expression and remove any prefix and suffix that matches the expression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b36e069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratulation'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "reg_stemmer=RegexpStemmer('ing$|ed$|s$|er$|ly$|al$', min=4)\n",
    "\n",
    "reg_stemmer.stem('congratulations')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf6520b",
   "metadata": {},
   "source": [
    "Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "401a232a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating --> eat\n",
      "eaten --> eaten\n",
      "eats --> eat\n",
      "programming --> program\n",
      "programmed --> program\n",
      "programmer --> programm\n",
      "program --> program\n",
      "finally --> final\n",
      "finalized --> final\n",
      "final --> final\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer=SnowballStemmer('english')\n",
    "\n",
    "for w in words:\n",
    "    print(w,'-->',stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "939bfa31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fair'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('fairly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ff0468",
   "metadata": {},
   "source": [
    "Lemmatization \n",
    "\n",
    "word into its root word instead of stem; a valid word is returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7f1a6cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "congratulations\n",
      "congratulation\n",
      "congratulations\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma=WordNetLemmatizer()\n",
    "\n",
    "# n-Noun\n",
    "#a-Adjective\n",
    "#verb-v\n",
    "#adverb-r\n",
    "\n",
    "\n",
    "print(lemma.lemmatize('congratulations',pos='a'))\n",
    "print(lemma.lemmatize('congratulations',pos='n'))\n",
    "print(lemma.lemmatize('congratulations',pos='v'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad00b321",
   "metadata": {},
   "source": [
    "Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e5cbb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=\"\"\" Pakistan,[f] officially the Islamic Republic of Pakistan,[g] is a country in South Asia. It is the fifth-most populous country, with a population of over 241.5 million,[d] having the second-largest Muslim population as of 2023. Islamabad is the nation's capital, while Karachi is its largest city and financial centre. Pakistan is the 33rd-largest country by area. Bounded by the Arabian Sea on the south, the Gulf of Oman on the southwest, and the Sir Creek on the southeast, it shares land borders with India to the east; Afghanistan to the west; Iran to the southwest; and China to the northeast. It shares a maritime border with Oman in the Gulf of Oman, and is separated from Tajikistan in the northwest by Afghanistan's narrow Wakhan Corridor.\n",
    "\n",
    "Pakistan is the site of several ancient cultures, including the 8,500-year-old Neolithic site of Mehrgarh in Balochistan, the Indus Valley Civilisation of the Bronze Age,[8] and the ancient Gandhara civilisation.[9] The regions that compose the modern state of Pakistan were the realm of multiple empires and dynasties, including the Achaemenid, the Maurya, the Kushan, the Gupta;[10] the Umayyad Caliphate in its southern regions, the Hindu Shahis, the Ghaznavids, the Delhi Sultanate, the Samma, the Shah Miris, the Mughals,[11] and finally, the British Raj from 1858 to 1947.\n",
    "\n",
    "Spurred by the Pakistan Movement, which sought a homeland for the Muslims of British India, and election victories in 1946 by the All-India Muslim League, Pakistan gained independence in 1947 after the partition of British India, which awarded separate statehood to its Muslim-majority regions and was accompanied by an unparalleled mass migration and loss of life.[12][13] Initially a Dominion of the British Commonwealth, Pakistan officially drafted its constitution in 1956, and emerged as a declared Islamic republic. In 1971, the exclave of East Pakistan seceded as the new country of Bangladesh after a nine-month-long civil war. In the following four decades, Pakistan has been ruled by governments that alternated between civilian and military, democratic and authoritarian, relatively secular and Islamist.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0152953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "stemmer=PorterStemmer()\n",
    "doc=nltk.sent_tokenize(paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e92c5f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pakistan , [ f ] offici islam republ pakistan , [ g ] countri south asia .', 'it fifth-most popul countri , popul 241.5 million , [ ] second-largest muslim popul 2023 .', \"islamabad nation 's capit , karachi largest citi financi centr .\", 'pakistan 33rd-largest countri area .', 'bound arabian sea south , gulf oman southwest , sir creek southeast , share land border india east ; afghanistan west ; iran southwest ; china northeast .', \"it share maritim border oman gulf oman , separ tajikistan northwest afghanistan 's narrow wakhan corridor .\", 'pakistan site sever ancient cultur , includ 8,500-year-old neolith site mehrgarh balochistan , indu valley civilis bronz age , [ 8 ] ancient gandhara civilis .', '[ 9 ] the region compos modern state pakistan realm multipl empir dynasti , includ achaemenid , maurya , kushan , gupta ; [ 10 ] umayyad caliph southern region , hindu shahi , ghaznavid , delhi sultan , samma , shah miri , mughal , [ 11 ] final , british raj 1858 1947 .', 'spur pakistan movement , sought homeland muslim british india , elect victori 1946 all-india muslim leagu , pakistan gain independ 1947 partit british india , award separ statehood muslim-major region accompani unparallel mass migrat loss life .', '[ 12 ] [ 13 ] initi dominion british commonwealth , pakistan offici draft constitut 1956 , emerg declar islam republ .', 'in 1971 , exclav east pakistan seced new countri bangladesh nine-month-long civil war .', 'in follow four decad , pakistan rule govern altern civilian militari , democrat authoritarian , rel secular islamist .']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(doc)):\n",
    "    words=nltk.word_tokenize(doc[i])\n",
    "    words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    doc[i]=\" \".join(words)\n",
    "\n",
    "print(doc)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907a6d98",
   "metadata": {},
   "source": [
    "Parts of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a0b3a037",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\mehno\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng',force=True, download_dir=r\"C:\\Users\\mehno\\AppData\\Roaming\\nltk_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7f00fd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('follow', 'VB'), ('four', 'CD'), ('decad', 'NN'), (',', ','), ('pakistan', 'JJ'), ('rule', 'NN'), ('govern', 'JJ'), ('altern', 'JJ'), ('civilian', 'JJ'), ('militari', 'NN'), (',', ','), ('democrat', 'NN'), ('authoritarian', 'JJ'), (',', ','), ('rel', 'JJ'), ('secular', 'JJ'), ('islamist', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(doc)):\n",
    "    words=nltk.word_tokenize(doc[i])\n",
    "    words=[word for word in words if word not in set(stopwords.words('english'))]\n",
    "    # doc[i]=\" \".join(words)\n",
    "\n",
    "pos=nltk.pos_tag(words)\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1c6729fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"\"\"Pakistan,[f] officially the Islamic Republic of Pakistan,[g] is a country in South Asia. It is the fifth-most populous country, with a population of over 241.5 million,[d] having the second-largest Muslim population as of 2023. Islamabad is the nation's capital, while Karachi is its largest city and financial centre. Pakistan is the 33rd-largest country by area. Bounded by the Arabian Sea on the south, the Gulf of Oman on the southwest, and the Sir Creek on the southeast, it shares land borders with India to the east; Afghanistan to the west; Iran to the southwest; and China to the northeast. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e967076f",
   "metadata": {},
   "source": [
    "POS tagging assigns a part of speech to each word in a sentence.\n",
    "\n",
    "Basically, it labels whether a word is a noun, verb, adjective, adverb, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2e0be1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pakistan,[f]', 'NN')]\n",
      "[('officially', 'RB')]\n",
      "[('the', 'DT')]\n",
      "[('Islamic', 'NNP')]\n",
      "[('Republic', 'JJ')]\n",
      "[('of', 'IN')]\n",
      "[('Pakistan,[g]', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('a', 'DT')]\n",
      "[('country', 'NN')]\n",
      "[('in', 'IN')]\n",
      "[('South', 'NNP')]\n",
      "[('Asia.', 'NN')]\n",
      "[('It', 'PRP')]\n",
      "[('is', 'VBZ')]\n",
      "[('the', 'DT')]\n",
      "[('fifth-most', 'NN')]\n",
      "[('populous', 'JJ')]\n",
      "[('country,', 'NN')]\n",
      "[('with', 'IN')]\n",
      "[('a', 'DT')]\n",
      "[('population', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('over', 'IN')]\n",
      "[('241.5', 'CD')]\n",
      "[('million,[d]', 'NN')]\n",
      "[('having', 'VBG')]\n",
      "[('the', 'DT')]\n",
      "[('second-largest', 'NN')]\n",
      "[('Muslim', 'NN')]\n",
      "[('population', 'NN')]\n",
      "[('as', 'IN')]\n",
      "[('of', 'IN')]\n",
      "[('2023.', 'CD')]\n",
      "[('Islamabad', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('the', 'DT')]\n",
      "[(\"nation's\", 'NN')]\n",
      "[('capital,', 'NN')]\n",
      "[('while', 'IN')]\n",
      "[('Karachi', 'NNP')]\n",
      "[('is', 'VBZ')]\n",
      "[('its', 'PRP$')]\n",
      "[('largest', 'JJS')]\n",
      "[('city', 'NN')]\n",
      "[('and', 'CC')]\n",
      "[('financial', 'JJ')]\n",
      "[('centre.', 'NN')]\n",
      "[('Pakistan', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('the', 'DT')]\n",
      "[('33rd-largest', 'JJ')]\n",
      "[('country', 'NN')]\n",
      "[('by', 'IN')]\n",
      "[('area.', 'NN')]\n",
      "[('Bounded', 'VBN')]\n",
      "[('by', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('Arabian', 'JJ')]\n",
      "[('Sea', 'NNP')]\n",
      "[('on', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('south,', 'NN')]\n",
      "[('the', 'DT')]\n",
      "[('Gulf', 'NNP')]\n",
      "[('of', 'IN')]\n",
      "[('Oman', 'NN')]\n",
      "[('on', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('southwest,', 'NN')]\n",
      "[('and', 'CC')]\n",
      "[('the', 'DT')]\n",
      "[('Sir', 'NNP')]\n",
      "[('Creek', 'NN')]\n",
      "[('on', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('southeast,', 'NN')]\n",
      "[('it', 'PRP')]\n",
      "[('shares', 'NNS')]\n",
      "[('land', 'NN')]\n",
      "[('borders', 'NNS')]\n",
      "[('with', 'IN')]\n",
      "[('India', 'NNP')]\n",
      "[('to', 'TO')]\n",
      "[('the', 'DT')]\n",
      "[('east;', 'NN')]\n",
      "[('Afghanistan', 'NNP')]\n",
      "[('to', 'TO')]\n",
      "[('the', 'DT')]\n",
      "[('west;', 'NN')]\n",
      "[('Iran', 'NN')]\n",
      "[('to', 'TO')]\n",
      "[('the', 'DT')]\n",
      "[('southwest;', 'NN')]\n",
      "[('and', 'CC')]\n",
      "[('China', 'NNP')]\n",
      "[('to', 'TO')]\n",
      "[('the', 'DT')]\n",
      "[('northeast.', 'NN')]\n",
      "[('It', 'PRP')]\n",
      "[('shares', 'NNS')]\n",
      "[('a', 'DT')]\n",
      "[('maritime', 'NN')]\n",
      "[('border', 'NN')]\n",
      "[('with', 'IN')]\n",
      "[('Oman', 'NN')]\n",
      "[('in', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('Gulf', 'NNP')]\n",
      "[('of', 'IN')]\n",
      "[('Oman,', 'NN')]\n",
      "[('and', 'CC')]\n",
      "[('is', 'VBZ')]\n",
      "[('separated', 'VBN')]\n",
      "[('from', 'IN')]\n",
      "[('Tajikistan', 'NN')]\n",
      "[('in', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('northwest', 'RB')]\n",
      "[('by', 'IN')]\n",
      "[(\"Afghanistan's\", 'NN')]\n",
      "[('narrow', 'NN')]\n",
      "[('Wakhan', 'NN')]\n",
      "[('Corridor.', 'NN')]\n",
      "[('Pakistan', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('the', 'DT')]\n",
      "[('site', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('several', 'JJ')]\n",
      "[('ancient', 'NN')]\n",
      "[('cultures,', 'NN')]\n",
      "[('including', 'VBG')]\n",
      "[('the', 'DT')]\n",
      "[('8,500-year-old', 'JJ')]\n",
      "[('Neolithic', 'JJ')]\n",
      "[('site', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('Mehrgarh', 'NN')]\n",
      "[('in', 'IN')]\n",
      "[('Balochistan,', 'NN')]\n",
      "[('the', 'DT')]\n",
      "[('Indus', 'NN')]\n",
      "[('Valley', 'NNP')]\n",
      "[('Civilisation', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('Bronze', 'NN')]\n",
      "[('Age,[8]', 'NN')]\n",
      "[('and', 'CC')]\n",
      "[('the', 'DT')]\n",
      "[('ancient', 'NN')]\n",
      "[('Gandhara', 'NN')]\n",
      "[('civilisation.[9]', 'NN')]\n",
      "[('The', 'DT')]\n",
      "[('regions', 'NNS')]\n",
      "[('that', 'IN')]\n",
      "[('compose', 'NN')]\n",
      "[('the', 'DT')]\n",
      "[('modern', 'JJ')]\n",
      "[('state', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('Pakistan', 'NN')]\n",
      "[('were', 'VBD')]\n",
      "[('the', 'DT')]\n",
      "[('realm', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('multiple', 'NN')]\n",
      "[('empires', 'NNS')]\n",
      "[('and', 'CC')]\n",
      "[('dynasties,', 'NN')]\n",
      "[('including', 'VBG')]\n",
      "[('the', 'DT')]\n",
      "[('Achaemenid,', 'NN')]\n",
      "[('the', 'DT')]\n",
      "[('Maurya,', 'NN')]\n",
      "[('the', 'DT')]\n",
      "[('Kushan,', 'NNP')]\n",
      "[('the', 'DT')]\n",
      "[('Gupta;[10]', 'NN')]\n",
      "[('the', 'DT')]\n",
      "[('Umayyad', 'NN')]\n",
      "[('Caliphate', 'NN')]\n",
      "[('in', 'IN')]\n",
      "[('its', 'PRP$')]\n",
      "[('southern', 'JJ')]\n",
      "[('regions,', 'NN')]\n",
      "[('the', 'DT')]\n",
      "[('Hindu', 'NN')]\n",
      "[('Shahis,', 'NN')]\n",
      "[('the', 'DT')]\n",
      "[('Ghaznavids,', 'NN')]\n",
      "[('the', 'DT')]\n",
      "[('Delhi', 'NN')]\n",
      "[('Sultanate,', 'NN')]\n",
      "[('the', 'DT')]\n",
      "[('Samma,', 'NN')]\n",
      "[('the', 'DT')]\n",
      "[('Shah', 'NN')]\n",
      "[('Miris,', 'NN')]\n",
      "[('the', 'DT')]\n",
      "[('Mughals,[11]', 'NN')]\n",
      "[('and', 'CC')]\n",
      "[('finally,', 'NN')]\n",
      "[('the', 'DT')]\n",
      "[('British', 'JJ')]\n",
      "[('Raj', 'NN')]\n",
      "[('from', 'IN')]\n",
      "[('1858', 'CD')]\n",
      "[('to', 'TO')]\n",
      "[('1947.', 'CD')]\n",
      "[('Spurred', 'VBN')]\n",
      "[('by', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('Pakistan', 'NN')]\n",
      "[('Movement,', 'NN')]\n",
      "[('which', 'WDT')]\n",
      "[('sought', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('homeland', 'NN')]\n",
      "[('for', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('Muslims', 'NNS')]\n",
      "[('of', 'IN')]\n",
      "[('British', 'JJ')]\n",
      "[('India,', 'NN')]\n",
      "[('and', 'CC')]\n",
      "[('election', 'NN')]\n",
      "[('victories', 'NNS')]\n",
      "[('in', 'IN')]\n",
      "[('1946', 'CD')]\n",
      "[('by', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('All-India', 'NNP')]\n",
      "[('Muslim', 'NN')]\n",
      "[('League,', 'NN')]\n",
      "[('Pakistan', 'NN')]\n",
      "[('gained', 'VBN')]\n",
      "[('independence', 'NN')]\n",
      "[('in', 'IN')]\n",
      "[('1947', 'CD')]\n",
      "[('after', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('partition', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('British', 'JJ')]\n",
      "[('India,', 'NN')]\n",
      "[('which', 'WDT')]\n",
      "[('awarded', 'VBN')]\n",
      "[('separate', 'JJ')]\n",
      "[('statehood', 'NN')]\n",
      "[('to', 'TO')]\n",
      "[('its', 'PRP$')]\n",
      "[('Muslim-majority', 'NN')]\n",
      "[('regions', 'NNS')]\n",
      "[('and', 'CC')]\n",
      "[('was', 'VBD')]\n",
      "[('accompanied', 'VBN')]\n",
      "[('by', 'IN')]\n",
      "[('an', 'DT')]\n",
      "[('unparalleled', 'JJ')]\n",
      "[('mass', 'NN')]\n",
      "[('migration', 'NN')]\n",
      "[('and', 'CC')]\n",
      "[('loss', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('life.[12][13]', 'NN')]\n",
      "[('Initially', 'RB')]\n",
      "[('a', 'DT')]\n",
      "[('Dominion', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('British', 'JJ')]\n",
      "[('Commonwealth,', 'NN')]\n",
      "[('Pakistan', 'NN')]\n",
      "[('officially', 'RB')]\n",
      "[('drafted', 'VBN')]\n",
      "[('its', 'PRP$')]\n",
      "[('constitution', 'NN')]\n",
      "[('in', 'IN')]\n",
      "[('1956,', 'CD')]\n",
      "[('and', 'CC')]\n",
      "[('emerged', 'VBD')]\n",
      "[('as', 'IN')]\n",
      "[('a', 'DT')]\n",
      "[('declared', 'VBN')]\n",
      "[('Islamic', 'NNP')]\n",
      "[('republic.', 'NN')]\n",
      "[('In', 'IN')]\n",
      "[('1971,', 'CD')]\n",
      "[('the', 'DT')]\n",
      "[('exclave', 'VB')]\n",
      "[('of', 'IN')]\n",
      "[('East', 'NN')]\n",
      "[('Pakistan', 'NN')]\n",
      "[('seceded', 'VBD')]\n",
      "[('as', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('new', 'JJ')]\n",
      "[('country', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('Bangladesh', 'NNP')]\n",
      "[('after', 'IN')]\n",
      "[('a', 'DT')]\n",
      "[('nine-month-long', 'JJ')]\n",
      "[('civil', 'JJ')]\n",
      "[('war.', 'NN')]\n",
      "[('In', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('following', 'VBG')]\n",
      "[('four', 'CD')]\n",
      "[('decades,', 'NN')]\n",
      "[('Pakistan', 'NN')]\n",
      "[('has', 'VBZ')]\n",
      "[('been', 'VBN')]\n",
      "[('ruled', 'VBN')]\n",
      "[('by', 'IN')]\n",
      "[('governments', 'NNS')]\n",
      "[('that', 'IN')]\n",
      "[('alternated', 'VBN')]\n",
      "[('between', 'IN')]\n",
      "[('civilian', 'JJ')]\n",
      "[('and', 'CC')]\n",
      "[('military,', 'NN')]\n",
      "[('democratic', 'JJ')]\n",
      "[('and', 'CC')]\n",
      "[('authoritarian,', 'NN')]\n",
      "[('relatively', 'RB')]\n",
      "[('secular', 'NN')]\n",
      "[('and', 'CC')]\n",
      "[('Islamist.', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "for i in word:\n",
    "    tag_elements=nltk.pos_tag([i])\n",
    "    print(tag_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bba12d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word=nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5b57f2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged=nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "71d1164e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\mehno\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker_tab',force=True, download_dir=r\"C:\\Users\\mehno\\AppData\\Roaming\\nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dcebe4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\mehno\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words', download_dir=r\"C:\\Users\\mehno\\AppData\\Roaming\\nltk_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6884e4be",
   "metadata": {},
   "source": [
    "NER finds important “named” things in text, like:\n",
    "\n",
    "Person names → \"Elon Musk\"\n",
    "\n",
    "Organizations → \"NASA\"\n",
    "\n",
    "Locations → \"Paris\"\n",
    "\n",
    "Dates → \"January 1, 2025\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6051f8f0",
   "metadata": {},
   "source": [
    "Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "eccc3e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.ne_chunk(tagged).draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365aad78",
   "metadata": {},
   "source": [
    "One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84c8e39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 1 2 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label=LabelEncoder()\n",
    "\n",
    "labels=['positive','negative','neutral','positive','negative']\n",
    "x=label.fit_transform(labels)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1668cf75",
   "metadata": {},
   "source": [
    "Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "017fff9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.4.0-cp311-cp311-win_amd64.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\mehno\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (2.3.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\mehno\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (1.16.3)\n",
      "Collecting smart_open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\mehno\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
      "Downloading gensim-4.4.0-cp311-cp311-win_amd64.whl (24.4 MB)\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 2.4/24.4 MB 9.6 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 3.1/24.4 MB 4.4 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 4.2/24.4 MB 2.0 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 5.8/24.4 MB 2.3 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 7.1/24.4 MB 2.5 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 9.2/24.4 MB 3.0 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 11.5/24.4 MB 3.3 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 12.1/24.4 MB 3.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 12.6/24.4 MB 3.2 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 12.8/24.4 MB 3.1 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 13.1/24.4 MB 3.1 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 14.2/24.4 MB 3.1 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 14.7/24.4 MB 3.1 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 15.7/24.4 MB 3.1 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 16.5/24.4 MB 3.0 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 17.3/24.4 MB 3.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 18.4/24.4 MB 3.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 18.9/24.4 MB 3.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 19.9/24.4 MB 3.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.7/24.4 MB 3.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.2/24.4 MB 2.9 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.8/24.4 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.5/24.4 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.1/24.4 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.9/24.4 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.4 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.4/24.4 MB 2.8 MB/s  0:00:08\n",
      "Downloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
      "Installing collected packages: smart_open, gensim\n",
      "\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   ---------------------------------------- 2/2 [gensim]\n",
      "\n",
      "Successfully installed gensim-4.4.0 smart_open-7.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b8ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63979707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.67187500e-01 -1.21582031e-01  2.85156250e-01  8.15429688e-02\n",
      "  3.19824219e-02 -3.19824219e-02  1.34765625e-01 -2.73437500e-01\n",
      "  9.46044922e-03 -1.07421875e-01  2.48046875e-01 -6.05468750e-01\n",
      "  5.02929688e-02  2.98828125e-01  9.57031250e-02  1.39648438e-01\n",
      " -5.41992188e-02  2.91015625e-01  2.85156250e-01  1.51367188e-01\n",
      " -2.89062500e-01 -3.46679688e-02  1.81884766e-02 -3.92578125e-01\n",
      "  2.46093750e-01  2.51953125e-01 -9.86328125e-02  3.22265625e-01\n",
      "  4.49218750e-01 -1.36718750e-01 -2.34375000e-01  4.12597656e-02\n",
      " -2.15820312e-01  1.69921875e-01  2.56347656e-02  1.50146484e-02\n",
      " -3.75976562e-02  6.95800781e-03  4.00390625e-01  2.09960938e-01\n",
      "  1.17675781e-01 -4.19921875e-02  2.34375000e-01  2.03125000e-01\n",
      " -1.86523438e-01 -2.46093750e-01  3.12500000e-01 -2.59765625e-01\n",
      " -1.06933594e-01  1.04003906e-01 -1.79687500e-01  5.71289062e-02\n",
      " -7.41577148e-03 -5.59082031e-02  7.61718750e-02 -4.14062500e-01\n",
      " -3.65234375e-01 -3.35937500e-01 -1.54296875e-01 -2.39257812e-01\n",
      " -3.73046875e-01  2.27355957e-03 -3.51562500e-01  8.64257812e-02\n",
      "  1.26953125e-01  2.21679688e-01 -9.86328125e-02  1.08886719e-01\n",
      "  3.65234375e-01 -5.66406250e-02  5.66406250e-02 -1.09375000e-01\n",
      " -1.66992188e-01 -4.54101562e-02 -2.00195312e-01 -1.22558594e-01\n",
      "  1.31835938e-01 -1.31835938e-01  1.03027344e-01 -3.41796875e-01\n",
      " -1.57226562e-01  2.04101562e-01  4.39453125e-02  2.44140625e-01\n",
      " -3.19824219e-02  3.20312500e-01 -4.41894531e-02  1.08398438e-01\n",
      " -4.98046875e-02 -9.52148438e-03  2.46093750e-01 -5.59082031e-02\n",
      "  4.07714844e-02 -1.78222656e-02 -2.95410156e-02  1.65039062e-01\n",
      "  5.03906250e-01 -2.81250000e-01  9.81445312e-02  1.80664062e-02\n",
      " -1.83593750e-01  2.53906250e-01  2.25585938e-01  1.63574219e-02\n",
      "  1.81640625e-01  1.38671875e-01  3.33984375e-01  1.39648438e-01\n",
      "  1.45874023e-02 -2.89306641e-02 -8.39843750e-02  1.50390625e-01\n",
      "  1.67968750e-01  2.28515625e-01  3.59375000e-01  1.22558594e-01\n",
      " -3.28125000e-01 -1.56250000e-01  2.77343750e-01  1.77001953e-02\n",
      " -1.46484375e-01 -4.51660156e-03 -4.46777344e-02  1.75781250e-01\n",
      " -3.75000000e-01  1.16699219e-01 -1.39648438e-01  2.55859375e-01\n",
      " -1.96289062e-01 -2.57568359e-02 -5.41992188e-02 -2.51464844e-02\n",
      " -1.93359375e-01 -3.17382812e-02 -8.74023438e-02 -1.32812500e-01\n",
      " -2.12402344e-02  4.33593750e-01 -5.20019531e-02  3.46679688e-02\n",
      "  8.00781250e-02  3.41796875e-02  1.99218750e-01 -2.39257812e-02\n",
      " -2.37304688e-01  1.93359375e-01  7.32421875e-02 -2.87109375e-01\n",
      "  1.25000000e-01  8.44726562e-02  1.30859375e-01 -2.19726562e-01\n",
      " -1.61132812e-01 -2.63671875e-01 -5.46875000e-01 -2.96875000e-01\n",
      "  3.44238281e-02 -2.87109375e-01 -1.93359375e-01 -1.61132812e-01\n",
      " -3.84765625e-01 -2.14843750e-01 -6.22558594e-03 -1.27929688e-01\n",
      " -1.00097656e-01 -6.21093750e-01  3.78906250e-01 -4.58984375e-01\n",
      "  1.44531250e-01 -9.13085938e-02 -3.08593750e-01  2.23632812e-01\n",
      "  7.86132812e-02 -2.16796875e-01  8.78906250e-02 -1.66992188e-01\n",
      "  1.14746094e-02 -2.53906250e-01 -6.25000000e-02  6.04248047e-03\n",
      "  1.56250000e-01  4.37500000e-01 -2.23632812e-01 -2.32421875e-01\n",
      "  2.75390625e-01  2.39257812e-01  4.49218750e-02 -7.51953125e-02\n",
      "  5.74218750e-01 -2.61230469e-02 -1.21582031e-01  2.44140625e-01\n",
      " -3.37890625e-01  8.59375000e-02 -7.71484375e-02  4.85839844e-02\n",
      "  1.43554688e-01  4.25781250e-01 -4.29687500e-02 -1.08398438e-01\n",
      "  1.19628906e-01 -1.91406250e-01 -2.12890625e-01 -2.87109375e-01\n",
      " -1.14746094e-01 -2.04101562e-01 -2.06298828e-02 -2.53906250e-01\n",
      "  8.25195312e-02 -3.97949219e-02 -1.57226562e-01  1.34765625e-01\n",
      "  2.08007812e-01 -1.78710938e-01 -2.00195312e-02 -8.34960938e-02\n",
      " -1.20605469e-01  4.29687500e-02 -1.94335938e-01 -1.32812500e-01\n",
      " -2.17285156e-02 -2.35351562e-01 -3.63281250e-01  1.51367188e-01\n",
      "  9.32617188e-02  1.63085938e-01  1.02050781e-01 -4.27734375e-01\n",
      "  2.83203125e-01  2.74658203e-04 -3.20312500e-01  1.68457031e-02\n",
      "  4.06250000e-01 -5.24902344e-02  7.91015625e-02 -1.41601562e-01\n",
      "  5.27343750e-01 -1.26953125e-01  4.74609375e-01 -6.64062500e-02\n",
      "  3.41796875e-01 -1.78710938e-01  3.69140625e-01 -2.05078125e-01\n",
      "  5.82885742e-03 -1.84570312e-01 -8.88671875e-02 -1.81640625e-01\n",
      " -4.80957031e-02  4.39453125e-01  2.12890625e-01 -3.07617188e-02\n",
      "  9.32617188e-02  2.40234375e-01  2.39257812e-01  2.51953125e-01\n",
      " -1.98974609e-02  1.24511719e-01 -4.73632812e-02 -2.13623047e-02\n",
      "  3.12500000e-02  3.05175781e-02  2.79296875e-01  9.08203125e-02\n",
      " -2.02148438e-01 -2.19726562e-02 -2.63671875e-01  8.78906250e-02\n",
      " -1.07421875e-01 -2.49023438e-01 -1.22070312e-02  1.73828125e-01\n",
      " -9.91210938e-02  7.27539062e-02  2.59765625e-01 -4.60937500e-01\n",
      "  3.59375000e-01 -2.25585938e-01  1.87988281e-02 -2.19726562e-01\n",
      " -2.08984375e-01 -1.51367188e-01  8.64257812e-02  1.11694336e-02\n",
      "  6.93359375e-02 -2.99072266e-02  1.43554688e-01  1.89453125e-01\n",
      " -1.32812500e-01  4.72656250e-01 -1.40625000e-01 -2.52685547e-02\n",
      "  1.91406250e-01 -2.63671875e-01 -1.39648438e-01  1.09375000e-01\n",
      "  1.97753906e-02  2.49023438e-01 -1.42578125e-01  4.15039062e-02]\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "wv=api.load('word2vec-google-news-300')\n",
    "vec=wv['cricket']\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73922fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
